{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b9d777-1e1d-4394-9f01-486d5e9014c2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "301f41cf-4120-4706-a4e9-e3656f5cd21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d72543-9175-4833-a925-8b6c4975d677",
   "metadata": {},
   "source": [
    "### Step 1: Download & Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a20c37-20db-4c77-941f-a9bb62e4d836",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\"\n",
    "root = \"./images\"\n",
    "download_and_extract_archive(url, download_root=root)\n",
    "root_dir = os.path.join(root, \"PennFudanPed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27164a3-f64f-4379-8d92-544d93696c76",
   "metadata": {},
   "source": [
    "**The dataset subdivides into two folders: annotations (which contains txt files of bounding box start and end points) and images**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4448a013-1b00-443d-9c28-b0dde525f948",
   "metadata": {},
   "source": [
    "### custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea000ee-139d-4236-84d8-c7c178e1f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PennFudanDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for PennFudanPed.\n",
    "\n",
    "    Each sample contains:\n",
    "        - image (RGB)\n",
    "        - bounding boxes (normalized)\n",
    "        - labels (1 = person)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.img_dir = os.path.join(root_dir, \"PNGImages\")\n",
    "        self.ann_dir = os.path.join(root_dir, \"Annotation\")\n",
    "        self.transform = transform\n",
    "\n",
    "        # Sort file lists to align images and annotations\n",
    "        self.imgs = sorted([f for f in os.listdir(self.img_dir) if f.endswith(\".png\")])\n",
    "        self.anns = sorted([f for f in os.listdir(self.ann_dir) if f.endswith(\".txt\")])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.imgs[idx])\n",
    "        ann_path = os.path.join(self.ann_dir, self.anns[idx])\n",
    "\n",
    "        # Load image\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "\n",
    "        # Parse annotation bounding boxes\n",
    "        with open(ann_path, \"r\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        boxes = re.findall(r\"\\((\\d+), (\\d+)\\) - \\((\\d+), (\\d+)\\)\", text)\n",
    "        boxes = np.array([[int(x1), int(y1), int(x2), int(y2)] for x1, y1, x2, y2 in boxes], dtype=np.float32)\n",
    "\n",
    "        # Normalize bounding boxes (for training stability)\n",
    "        boxes[:, [0, 2]] /= w\n",
    "        boxes[:, [1, 3]] /= h\n",
    "\n",
    "        # Label = person (single class)\n",
    "        labels = np.ones((len(boxes),), dtype=np.int64)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, torch.tensor(boxes, dtype=torch.float32), torch.tensor(labels, dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa8cd1-1a38-4080-8c86-18aafb9baf73",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc56b564-4ce6-49b7-88d5-3a790f8288eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = PennFudanDataset(root_dir, transform=transform)\n",
    "train_idx, test_idx = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "test_subset = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, boxes, labels = zip(*batch)\n",
    "    return list(imgs), list(boxes), list(labels)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_subset, batch_size=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755ba88c-f0d3-46ca-981b-8c33779ffafe",
   "metadata": {},
   "source": [
    "Coming to the model we‚Äôll be needing for this project, we need to keep two things in mind. First, to avoid additional hassle and for efficient feature extraction, we‚Äôll use a pre-trained model to act as the base model. Second, the base model will then be split into two parts; the box regressor and the label classifier. Both of these will be individual model entities.\n",
    "\n",
    "The second thing to remember is that only the box regressor and the label classifier will have trainable weights. The weights of the pre-trained model will be left untouched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b70d3-15da-4524-abdf-623774007dc0",
   "metadata": {},
   "source": [
    "### Define Object Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a0f9e78-4f67-4faf-b91f-173a1ed1927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepObjectDetector(nn.Module):\n",
    "    def __init__(self, baseModel, numClasses):\n",
    "        super(DeepObjectDetector, self).__init__()\n",
    "        self.baseModel = baseModel\n",
    "        self.baseModel.fc = nn.Identity()  # remove final classifier\n",
    "        feature_dim = 2048\n",
    "\n",
    "        # üî∏ Deeper regressor\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # üî∏ Deeper classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, numClasses)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.baseModel(x)\n",
    "        boxes = self.regressor(features)\n",
    "        classes = self.classifier(features)\n",
    "        return boxes, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0225a28c-2326-44f4-9486-b43cdc4294ec",
   "metadata": {},
   "source": [
    "###  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cb8de29-e9bb-4e08-a81b-376afcf0f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeepObjectDetector(models.resnet50(weights=\"IMAGENET1K_V1\"), numClasses=2).to(device)\n",
    "\n",
    "class_loss_fn = nn.CrossEntropyLoss()\n",
    "bbox_loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c462da1-ad50-4e8c-b3d2-a61bbbc76c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Starting Training...\n",
      "\n",
      "Epoch [1/5] ‚ûú Loss: 0.4205\n",
      "Epoch [2/5] ‚ûú Loss: 0.0645\n",
      "Epoch [3/5] ‚ûú Loss: 0.0621\n",
      "Epoch [4/5] ‚ûú Loss: 0.0601\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "train_losses = []\n",
    "\n",
    "print(\"\\nüîÅ Starting Training...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imgs, boxes, labels in train_loader:\n",
    "        for img, box, label in zip(imgs, boxes, labels):\n",
    "            img, box, label = img.to(device), box.to(device), label.to(device)\n",
    "\n",
    "            pred_box, pred_class = model(img.unsqueeze(0))\n",
    "\n",
    "            # losses\n",
    "            bbox_loss = bbox_loss_fn(pred_box, box.mean(dim=0, keepdim=True))\n",
    "            class_loss = class_loss_fn(pred_class, label[0].unsqueeze(0))\n",
    "            loss = bbox_loss + class_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] ‚ûú Loss: {avg_loss:.4f}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a62d1-bb55-4a3d-a739-34fb64244303",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b66a374-d508-4f1e-93d1-97b42199af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(img):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    return img * std + mean\n",
    "\n",
    "\n",
    "def denormalize_box(box, width, height):\n",
    "    box = box.clone()\n",
    "    box[0::2] *= width\n",
    "    box[1::2] *= height\n",
    "    return box\n",
    "\n",
    "\n",
    "def draw_label(pil_img, box, label):\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "    font = ImageFont.load_default()\n",
    "    text_w, text_h = draw.textlength(label, font=font), 12\n",
    "    x1, y1, x2, y2 = box\n",
    "    draw.rectangle([x1, max(0, y1 - text_h - 2), x1 + text_w + 4, y1], fill=\"white\")\n",
    "    draw.text((x1 + 2, y1 - text_h - 2), label, fill=\"black\", font=font)\n",
    "    return pil_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d6a035-e445-4e4f-8f0e-bba36e61f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, data_loader, device, num_images=5):\n",
    "    model.eval()\n",
    "    shown = 0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, boxes, labels in data_loader:\n",
    "            for img, box, label in zip(imgs, boxes, labels):\n",
    "                img, label = img.to(device), label.to(device)\n",
    "\n",
    "                pred_box, pred_class = model(img.unsqueeze(0))\n",
    "                pred_box = pred_box[0].cpu()\n",
    "                class_id = pred_class.argmax(1).item()\n",
    "                conf = torch.softmax(pred_class, dim=1)[0, class_id].item()\n",
    "\n",
    "                # Accuracy\n",
    "                total += 1\n",
    "                if class_id == label[0].item():\n",
    "                    correct += 1\n",
    "\n",
    "                # Unnormalize + visualize\n",
    "                img_disp = unnormalize(img.cpu()).clamp(0, 1)\n",
    "                img_disp = (img_disp * 255).byte()\n",
    "                _, H, W = img_disp.shape\n",
    "\n",
    "                pred_box_abs = denormalize_box(pred_box, W, H).unsqueeze(0)\n",
    "                gt_boxes = torch.stack([denormalize_box(b, W, H) for b in box])\n",
    "\n",
    "                drawn = draw_bounding_boxes(img_disp, gt_boxes.to(torch.int32), colors=\"green\", width=2)\n",
    "                drawn = draw_bounding_boxes(drawn, pred_box_abs.to(torch.int32), colors=\"red\", width=3)\n",
    "\n",
    "                pil_img = F.to_pil_image(drawn)\n",
    "                label_text = f\"Pred: person ({conf:.2f})\"\n",
    "                pil_img = draw_label(pil_img, pred_box_abs[0].tolist(), label_text)\n",
    "\n",
    "                plt.figure(figsize=(6, 6))\n",
    "                plt.imshow(pil_img)\n",
    "                plt.title(\"üü© Ground Truth | üü• Prediction\")\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()\n",
    "\n",
    "                shown += 1\n",
    "                if shown >= num_images:\n",
    "                    acc = correct / total * 100\n",
    "                    print(f\"‚úÖ Shown Accuracy: {acc:.2f}%\")\n",
    "                    return\n",
    "\n",
    "    acc = correct / total * 100\n",
    "    print(f\"‚úÖ Overall Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40472b2f-0fbe-47b8-b6f1-e14ba639f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model, test_loader, device, num_images=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
